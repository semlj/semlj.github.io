---
title: "SEM: Political Democracy"
author: "(Marcello Gallucci)"
nickname: simplemodels
topic: general
category: example
output: 
  html_document:
     includes:
         in_header: ganalytics.txt
     toc: true
     toc_float:
        collapsed: false
bibliography: 
     - bib.bib
editor_options: 
  chunk_output_type: console
---

```{r echo=FALSE,results='hide'}
source("../R/constants.R")
source("../R/functions.R")
data<-read.csv("../data/worland5.csv")
```


`r keywords("SEM, structural equations model, lavaan")`

`r draft`

In this example show a simple structural equation model using a classical dataset from Bollen [@bollen1989]. This example can be found also in `lavaal` [website](https://lavaan.ugent.be/tutorial/sem.html), so the user can compare results and get information for `lavaan` tutorials.

We show input for  `r modulename` _interactive sub-module_. For `r modulename` _syntax sub-module_ one just paste the `lavaan` syntax and gets the same results of the interactive module.


# Research data

The dataset contains various measures of political democracy and industrialization in developing countries. For what concerns us, observed variables $y1$ to $y4$ refer to democracy indicator in 1960, observed variables $y5$ to $y8$ refer to democracy indicator in 1965, whereas indicators $x1$ to $x3$ are indistrialization indicators in 1960. 

The model we want to estimate is basically a mediation model in which (latent)  indistrialization level in 1960 influences (latent) democracy in 1960, which in turn influences (latent) democracy in 1965.

The path diagram should be the following.

`r pic("pics/example2/output_model1_path.png")`

It is worth noticing that this model requires the latent relations we mentioned, and some correlations between observed variables. In particular, we want the following pairs of observed variables to be correlated: 
$r(y1,y5)$, $r(y2,y4)$, $r(y2,y6)$, $r(y3,y7)$,$r(y4,y8)$, and $r(y6,y8)$. Those are correlations among residuals of the observed variables, after removing their loadings on the latent variables. In factor analysis terms, the correlations among uniquenesses.

`r pic("pics/example2/input_model1_variances.png")`


# Model Input

We need to define the three latent variables `ind60`, `dem60`, and `dem65`, each measured with the corresponding observed variables. 

For each latent variable, we define its name in place of "Endogenous 1", "Endogenous 2" etc, and put the observed variables in the corresponding field.

`r pic("pics/example2/input_model1_vars.png")`

In details, we have the variables role panels defined as follows:

`r pic("pics/example2/input_model1_vars2.png")`
`r pic("pics/example2/input_model1_vars3.png")`
`r pic("pics/example2/input_model1_vars4.png")`

When we set the variables like we did, `r modulename` knows our model has three latent variables, two endogenuous and one exogenous, and it knows what are the indicators of each latent variable.

We now need to specify the relationships between variables. `dem60` is predicted by `ind60`, and `dem65` is predicted by `ind60` and `dem60`. We declare that in the `r opt("Endogenous models")`.

`r pic("pics/example2/input_model1_model.png")`

If we run this model like this, we would obtain the following structural model. 

`r pic("pics/example2/output_model1_path2.png")`

We can notice that this model, as compared with the intented model seen before, lacks the correlations between the observed variables. We can easily free them (meaning estimate them), in the `r opt("Variances and covariances")` panel. We select the pairs of variables we wants to be correlated.

`r pic("pics/example2/input_model1_variances.png")`


Because we want to see the path diagram of the model, we ask for the path diagram in the `r opt("Path Diagram")` panel.

`r pic("pics/example1/input_model1_diagram.png")`

## Output

## Path diagram

We check the path diagram to be sure we set the model as we intended, and we did.

`r pic("pics/example2/output_model1_path3.png")`

For now on, we discuss the output tables in the order they appear in the output.

## Model info

Here we find general informations about the model.

`r pic("pics/example2/output_model1_modelinfo.png")`

We can see that we have 75 observations, 31 free (estimated) parameters, and that the model converged. We find the model likelihood for the user model and the unrestricted one, and the syntax we input (better check it's right). The likelihood computations are usefull for advanced models comparisons, we can ignore them in this example.

## Overall Tests

Here we find inferential tests regarding the whole model. 

`r pic("pics/example2/output_model1_overalltests.png")`

To interpret the `r opt("Fit indices")` table, I'd suggest to use [David Kenny website](http://davidakenny.net/webinars/listpp.htm#Fit) that has clear and detailed information, and the `lavaal` [website](https://lavaan.ugent.be/tutorial/sem.html).

Here we comment on the `r opt("Model tests")`: `r opt("User Model")` $\chi^2$ and `r opt("Baseline Model")` $\chi^2$ have very different interpretations. The smaller the first test, the better is our the model, the largest the second, the more relationships are in the data.

*  `r opt("User Model")` $\chi^2$ tests the null-hypothesis that our model is equivalent to a fully-saturated model (all possible paths are estimated). Recall that if we do not estimate a path (or a loading, or a residual correlation), we are setting it to zero. Thus, this test is basically an overall test of significance of the paths we set to zero. Are some of the paths we did not estimate (set to zero) different from zero. If the $\chi^2$ is significant, at least some of them do not appear to be zero. Sample size, howevwer, may influence this test, so see [David Kenny comments](http://www.davidakenny.net/cm/fit.htm) on this issue. Technically, the test tests the null-hypothesis that the covariances among observed variables implied by the model are equal to the observed covariances. This means: a large $\chi^2$ (small p-value) indicates a mis-fit.

*  `r opt("Baseline Model")` $\chi^2$ tests the null-hypothesis that there are not relationships between each endogenous variable and any other variable in the model. Thus, this test is basically an overall test of significance that something is correlated with and among the endogenous variables. Technically, the test tests the null-hypothesis that the covariances among observed variables implied by a model in which the endogenous variables do not covariate with anything are equal to the covariances impled by a fully-saturated model. This means: a larger $\chi^2$ (small p-value) indicates more relationship to capture. 

This reasoning may get more complicated by means and intercepts, but in our model they are free to be estimated, so they do not participate in the model fit tests. 


The first table of interest here is the `r opt("Measurement model")` table. We see that the loading of the two observed variables on their respective latent variables are set to 1. This means that observed and latent are the same variables, so we can interpret the relationship between `l__read` and `l_motiv` as the relationship between `read` and `motive`.

This relationship (the B coefficient) is found in the `r opt("Parameters Estimates")` table.

`r pic("pics/example1/output_model1_parameters.png")`

The `Parameter Estimates` table reports the regression coefficients. Notice that the `Estimate` is the B coefficient, and the $\beta$ is the fully standardized coefficient. In this case they are equal because the variables have the same variance (100), which implies that B and $\beta$ are equal. This table basically shows that the regression coefficient linking `read` and `motiv` is $.530$, $z=14.0$,$p<.001$.

### Overall tests

`r pic("pics/example1/output_model1_overall.png")`

`Model tests` table reports the chi-square of the whole model. Because there is no fixed parameter, the user model chi-square is zero. The `Baseline model` row refers to the comparison between the user model, that is `l_read~l_motiv`, and a model in which the variables covariance is set to zero. In this case, it is simply testing that the effect of `motiv` on `read` is different from zero.

Fit indices are the standard indices reported is structural equation models. 

### $R^2$


To see the $R^2$ of this model, we select `r opt("Rsquared=Endogenous")` option in the `r opt("Output Options")` panel. 

`r pic("pics/example1/input_model1_r2.png")`

The $R^2$ one obtains is equivalent to the $R^2$ one would get in a linear regression.

`r pic("pics/example1/output_model1_r2.png")`


### Coefficients


### Additional parameters

`r pic("pics/example1/output_model1_variances.png")`


The `Variances and Covariances` table reports variables estimated parameters. As regards `l_read`, we obtain the estimated residual variance. This value ($71.8$) is basically the Mean Square Error ($SS_{error} / DF_{error}$) one gets in a standard regression. For `l_motiv`, we obtain the whole variance _estimated_ in the sample. Variances of the observed variables are set to 0 because all their variability is captured by the latent variables.

Finally, the path diagram which is indeed very simple 

`r pic("pics/example1/output_model1_path.png")`


# Multiple regression (Model 2)

## Input
In model 2 we add a second predictor `ppsych`, first in the variables role panel, then to the endogenous variables model.

`r pic("pics/example1/input_model2_vars.png")`

`r pic("pics/example1/input_model2_model.png")`


In `r modulename` _syntax sub-module_ we achieve the same model definition with `lavaan` syntax.

`r pic("pics/example1/syntax_model2_model.png")`


## Output


Results are in line with a standard multiple regression model. The path diagram is as follows

`r pic("pics/example1/output_model2_path.png")`

We do not go through the output again, because we would find the same tables with the same interpretation as before. However, we can do something with this model that OLS regression does not allow to do with simplicity. We test the null-hypothesis that the two predictors have the same effect. In other words, we test that the effect from `motiv` to `read` is not different from the effect from `ppsych` to `read`. 

Let's look at the parameters we have just estimated.

`r pic("pics/example1/output_model2_parameters.png")`

We want to test the null-hypothesis $.461=-.275$. This test makes sense because the two coefficients have the same scale, as indicated by the fact that `motiv` and `ppsych` have the same variance.

To obtain this test, we need to restrict the two coefficients to be equal. We can refer to parameters with their label (automatically assigned by `r modulename`). Parameters labels are shown in the tables when `r opt("Show parameters labels")` option is flagged in the `r opt("Output Options")` panel.

`r pic("pics/example1/output_model2_parameters2.png")`

We can see that the two coefficients we want to equate are labelled $p4$ and $p5$. 

Thus, in `r modulename` _interactive sub-module_ we go to the `r opt ("Custom Model Settings")` panel and constraints them as equal.

`r pic("pics/example1/input_model2_constraints.png")`

in `r modulename` _syntax sub-module_ we just write the contraints syntax below the model syntax.

`r pic("pics/example1/syntax_model2_constraints.png")`



The output now reports the coefficients as equal to .093. 

In the output we also find a new table, `Constraints Score Tests`, which provides a chi-square test comparing the original model with the constrained model with the two coefficients set as equal (_equal_ is defined as $==$)(more examples of contraints can be obtained `r modulename` by selecting `r opt("Show syntax examples")` option in the `r opt("Output Options")` panel). 

`r pic("pics/example1/output_model2_constraints.png")`


The test null-hypothesis is that they are equal, thus if one finds the test to be _significant_, one can reject the null-hypothesis. In this case, the coefficients can be considered as _statistically different_.

We the same logic, one can test any reasonable hypothesis in a model by restricting the appropriate coefficients to be equal or to equate a specific value. 

# Multivariate regression 

_(Models 3A-E)_

We can now add some endogenous dependent variables. We add `arith` (_arithmetic test score_) as dependent variable in the model, with `motiv` and `ppsych` as predictors of `read`, and only `motiv` as predictor of `arith`. The path diagram looks like this.

`r pic("pics/example1/output_model3_path.png")`


In `r modulename` _interactive sub-module_ we obtain this model by adding `l_arith` to the `r opt("Latent Endogeneous Variables")` field, filling it with the observed variable `arith`, and then defining `l_arith` predictors in the `r opt("Endogenous Models")` panel.

`r pic("pics/example1/input_model3_vars.png")`
`r pic("pics/example1/input_model3_model.png")`

In `r modulename` _syntax sub-module_ we add the latent variable updating the syntax.

`r pic("pics/example1/syntax_model3_model.png")`

We get exactly the same results obtained in lavaan (cf  [UCLA statistical consulting webpage](https://stats.idre.ucla.edu/r/seminars/rsem/)). 

`r pic("pics/example1/output_model3_parameters.png")`
`r pic("pics/example1/output_model3_variances.png")`

The $R^2$ are now two, one for each endogenous variable (see [PATHj example](https://pathj.github.io/example1.html) for a discussion about $R^2$ in multivariate path analysis)

`r pic("pics/example1/output_model3_r2.png")`


# Related examples
`r include_examples("general")`
`r include_examples("gui")`

`r issues()`

# Additional references